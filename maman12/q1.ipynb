{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sklearn.datasets as skds\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model and custom layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "new(): argument 'size' must be tuple of ints, but found element of type float at pos 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12932/1541267711.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m model = nn.Sequential(\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[0mMyCustomLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m392\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m         \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m )\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12932/1541267711.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, size_in, size_out)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize_in\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msize_in\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize_out\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize_in\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# nn.Parameter is a Tensor that's a module parameter.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: new(): argument 'size' must be tuple of ints, but found element of type float at pos 2"
     ]
    }
   ],
   "source": [
    "class MyCustomLayer(nn.Module):\n",
    "\tdef __init__(self, size_in, size_out):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tself.size_in, self.size_out = size_in, size_out\n",
    "\t\tself.weights = nn.Parameter(torch.Tensor(size_out, size_in))  # nn.Parameter is a Tensor that's a module parameter.\n",
    "\t\tself.bias = nn.Parameter(torch.Tensor(size_out))\n",
    "\n",
    "\t\tself.linear1 = nn.Linear(size_in, size_out)\n",
    "\t\tself.activation = nn.ReLU(inplace=True) # inplace = don't use extra memory\n",
    "\n",
    "\t\t# initialize weights and biases\n",
    "\t\tself.apply(self.weights_init_normal)\n",
    "\t\t\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\t# 1. X is batch of the samples, as input\n",
    "\t\t# Note to self: X is 2 dimensional: First dim is the batch size, second dim is the number of features!\n",
    "\t\t#x = torch.flatten(x) # We don't need to flatten, since dim=0 is the batch size, and we only care about features\n",
    "\n",
    "\t\t# 2. Split input into two tensors with same length (we can assume that the length of X is even)\n",
    "\t\tassert x.shape[0] % 2 == 0\n",
    "\t\tx1, x2 = torch.tensor_split(x, 2, dim=1) # Split the features dimension\n",
    "\t\tassert x1.size() == x2.size()\n",
    "\n",
    "\t\t# 3. Put two tensors into the same aggregation layer (linear, for example)\n",
    "\t\tx1 = self.linear1(x1)\n",
    "\t\tx2 = self.linear1(x2)\n",
    "\n",
    "\t\t# 3. Then put into activation layer\n",
    "\t\tx1 = self.activation(x1)\n",
    "\t\tx2 = self.activation(x2)\n",
    "\n",
    "\t\t# 4. Concatinate two halves to create output Y\n",
    "\t\t#Y = torch.cat((x1, x2))\n",
    "\t\t#print(Y.size())\n",
    "\t\tY = (x1+x2)/2\n",
    "\t\treturn Y\n",
    "\t\n",
    "\tdef weights_init_normal(self, m):\n",
    "\t\t'''Takes in a module and initializes all linear layers with weight values taken from a normal distribution.'''\n",
    "\n",
    "\t\tclassname = m.__class__.__name__\n",
    "\t\t# for every Linear layer in a model\n",
    "\t\tif classname.find('Linear') != -1:\n",
    "\t\t\ty = m.in_features\n",
    "\t\t\t# m.weight.data shoud be taken from a normal distribution\n",
    "\t\t\tm.weight.data.normal_(0.0,1/numpy.sqrt(y))\n",
    "\t\t\t# m.bias.data should be 0\n",
    "\t\t\tm.bias.data.fill_(0)\n",
    "\t\t\n",
    "\t\t\tprint(\"bias = \", m.bias.data)\n",
    "\t\t\tprint(\"weight = \", m.weight.data)\n",
    "\t\t\tprint(\"weight mean: \", m.weight.data.mean(), \"min: \", torch.min(m.weight.data), \"max: \", torch.max(m.weight.data))\n",
    "\n",
    "model = nn.Sequential(\n",
    "\tMyCustomLayer(392, 20),\n",
    "\tnn.Softmax()\n",
    ")\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize parameters - link\n",
    "\n",
    "https://stackoverflow.com/a/55546528/5854499\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_transformed = torchvision.datasets.FashionMNIST( \n",
    "#     root=\"/22961\", train=True, download=False, \n",
    "#     transform=torchvision.transforms.PILToTensor())\n",
    "# train_dataloader = DataLoader(train_data_transformed, batch_size=4)\n",
    "\n",
    "# trans=torchvision.transforms.Compose(\n",
    "#     [torchvision.transforms.PILToTensor(),\n",
    "#      torchvision.transforms.ConvertImageDtype(torch.float)])\n",
    "# train_data_transformed = torchvision.datasets.FashionMNIST(\n",
    "#     root=\"/22961\", train=True, download=True,\n",
    "#     transform=trans)\n",
    "\n",
    "dataset = torchvision.datasets.FashionMNIST(root=\"/22961\", train=True, download=True, transform=torchvision.transforms.PILToTensor())\n",
    "dataloader = DataLoader(dataset, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define cost and optimizer functions\n",
    "\n",
    "We use Negative Log Likelihood Loss (NLLLoss), which is our Cross Entropy function\n",
    "\n",
    "And we use classic SGD optimizer to find minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CE_loss=torch.nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define batch iterative function\n",
    "\n",
    "This function is called for each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_batch(idx):\n",
    "\timgs, labels = next(iter(dataloader))\n",
    "\timgs = imgs.flatten(start_dim=1)\n",
    "\toptimizer.zero_grad()\n",
    "\ty_model=model(imgs.float())\n",
    "\n",
    "\tloss=CE_loss(y_model,labels)\n",
    "\tloss.backward()\n",
    "\toptimizer.step()\n",
    "\n",
    "\tpredicted_labels = y_model.argmax(dim=1)\n",
    "\tacc = (predicted_labels == labels).sum()/len(labels)\n",
    "\treturn loss.detach(), acc.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "batches=len(dataloader)\n",
    "print(\"num of batches: \\n\", batches)\n",
    "batch_loss=torch.zeros(batches)\n",
    "batch_acc=torch.zeros(batches)\n",
    "for idx in tqdm(range(batches)):\n",
    "\tbatch_loss[idx], batch_acc[idx] = iterate_batch(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Block diagram of the NN\n",
    "\n",
    "![](./drawio/Untitled%20Diagram.drawio.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_params(model):\n",
    "    pp=0\n",
    "    for p in list(model.parameters()):\n",
    "        nn=1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(get_n_params(model))\n",
    "print(count_parameters(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual calculation\n",
    "\n",
    "```\n",
    "Hidden layer:\n",
    "\n",
    "392*20 + 392*20 = 7840 weights\n",
    "Also, we have 20 output neurons: 20 neurons means 20 biases (one bias for each Y_0, Y_1, ..., Y_19 equation)\n",
    "\n",
    "Activation layer:\n",
    "0 (no weights, no bias, ReLU is max(0, X) so no additional parameters except from the input to the function itself)\n",
    "\n",
    "\n",
    "Summary:\n",
    "7840 + 0 + 20 = 7860\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compared to built-in layers of PyTorch\n",
    "\n",
    "It's the same, but I calculated manually diffirently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_model = torch.nn.Sequential(\n",
    "\ttorch.nn.Linear(392, 20),\n",
    "\ttorch.nn.Softmax()\n",
    ")\n",
    "print(get_n_params(model))\n",
    "print(count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params+=params\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "\n",
    "count_parameters(model)\n",
    "print(model)\n",
    "\n",
    "count_parameters(torch_model)\n",
    "print(torch_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "491bb19b02bca4120e598d98c213a4bec2319d7b10161d80e88d4a71a12afc06"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
