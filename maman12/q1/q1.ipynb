{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sklearn.datasets as skds\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model and custom layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias =  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "weight =  tensor([[-9.5368e-02,  9.6251e-02,  5.0826e-03,  ...,  5.1594e-02,\n",
      "         -1.0958e-01,  2.5136e-02],\n",
      "        [-4.0485e-02,  5.3265e-02,  5.3008e-02,  ...,  1.1843e-02,\n",
      "          3.2150e-02,  4.6764e-02],\n",
      "        [ 1.4811e-03,  2.1794e-02, -2.0335e-02,  ..., -2.0085e-02,\n",
      "          6.2974e-02, -2.9049e-02],\n",
      "        ...,\n",
      "        [ 4.0089e-02, -3.0543e-02,  9.3759e-02,  ...,  8.1432e-02,\n",
      "         -4.5846e-02, -6.9860e-02],\n",
      "        [ 5.9842e-02,  5.7093e-03,  5.6555e-03,  ..., -3.9907e-03,\n",
      "          2.1229e-02,  6.9627e-02],\n",
      "        [ 5.6902e-05, -3.7168e-02,  1.8137e-02,  ..., -1.7993e-03,\n",
      "         -9.8354e-02, -5.4815e-03]])\n",
      "weight mean:  tensor(-0.0004) min:  tensor(-0.2025) max:  tensor(0.1838)\n",
      "Sequential(\n",
      "  (0): MyCustomLayer(\n",
      "    (linear1): Linear(in_features=392, out_features=20, bias=True)\n",
      "    (activation): ReLU(inplace=True)\n",
      "  )\n",
      "  (1): Softmax(dim=None)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MyCustomLayer(nn.Module):\n",
    "\tdef __init__(self, size_in, size_out):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tself.size_in, self.size_out = size_in, size_out\n",
    "\t\tself.weights = nn.Parameter(torch.Tensor(size_out, size_in))  # nn.Parameter is a Tensor that's a module parameter.\n",
    "\t\tself.bias = nn.Parameter(torch.Tensor(size_out))\n",
    "\n",
    "\t\tself.linear1 = nn.Linear(size_in, size_out)\n",
    "\t\tself.activation = nn.ReLU(inplace=True) # inplace = don't use extra memory\n",
    "\n",
    "\t\t# initialize weights and biases\n",
    "\t\tself.apply(self.weights_init_normal)\n",
    "\t\t\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\t# 1. X is batch of the samples, as input\n",
    "\t\t# Note to self: X is 2 dimensional: First dim is the batch size, second dim is the number of features!\n",
    "\t\t#x = torch.flatten(x) # We don't need to flatten, since dim=0 is the batch size, and we only care about features\n",
    "\n",
    "\t\t# 2. Split input into two tensors with same length (we can assume that the length of X is even)\n",
    "\t\tassert x.shape[0] % 2 == 0\n",
    "\t\tx1, x2 = torch.tensor_split(x, 2, dim=1) # Split the features dimension\n",
    "\t\tassert x1.size() == x2.size()\n",
    "\n",
    "\t\t# 3. Put two tensors into the same aggregation layer (linear, for example)\n",
    "\t\tx1 = self.linear1(x1)\n",
    "\t\tx2 = self.linear1(x2)\n",
    "\n",
    "\t\t# 3. Then put into activation layer\n",
    "\t\tx1 = self.activation(x1)\n",
    "\t\tx2 = self.activation(x2)\n",
    "\n",
    "\t\t# 4. Concatinate two halves to create output Y\n",
    "\t\t#Y = torch.cat((x1, x2))\n",
    "\t\t#print(Y.size())\n",
    "\t\tY = (x1+x2)/2\n",
    "\t\treturn Y\n",
    "\t\n",
    "\tdef weights_init_normal(self, m):\n",
    "\t\t'''Takes in a module and initializes all linear layers with weight values taken from a normal distribution.'''\n",
    "\n",
    "\t\tclassname = m.__class__.__name__\n",
    "\t\t# for every Linear layer in a model\n",
    "\t\tif classname.find('Linear') != -1:\n",
    "\t\t\ty = m.in_features\n",
    "\t\t\t# m.weight.data shoud be taken from a normal distribution\n",
    "\t\t\tm.weight.data.normal_(0.0,1/numpy.sqrt(y))\n",
    "\t\t\t# m.bias.data should be 0\n",
    "\t\t\tm.bias.data.fill_(0)\n",
    "\t\t\n",
    "\t\t\tprint(\"bias = \", m.bias.data)\n",
    "\t\t\tprint(\"weight = \", m.weight.data)\n",
    "\t\t\tprint(\"weight mean: \", m.weight.data.mean(), \"min: \", torch.min(m.weight.data), \"max: \", torch.max(m.weight.data))\n",
    "\n",
    "model = nn.Sequential(\n",
    "\tMyCustomLayer(392, 20),\n",
    "\tnn.Softmax()\n",
    ")\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize parameters - link\n",
    "\n",
    "https://stackoverflow.com/a/55546528/5854499\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_transformed = torchvision.datasets.FashionMNIST( \n",
    "#     root=\"/22961\", train=True, download=False, \n",
    "#     transform=torchvision.transforms.PILToTensor())\n",
    "# train_dataloader = DataLoader(train_data_transformed, batch_size=4)\n",
    "\n",
    "# trans=torchvision.transforms.Compose(\n",
    "#     [torchvision.transforms.PILToTensor(),\n",
    "#      torchvision.transforms.ConvertImageDtype(torch.float)])\n",
    "# train_data_transformed = torchvision.datasets.FashionMNIST(\n",
    "#     root=\"/22961\", train=True, download=True,\n",
    "#     transform=trans)\n",
    "\n",
    "dataset = torchvision.datasets.FashionMNIST(root=\"/22961\", train=True, download=True, transform=torchvision.transforms.PILToTensor())\n",
    "dataloader = DataLoader(dataset, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define cost and optimizer functions\n",
    "\n",
    "We use Negative Log Likelihood Loss (NLLLoss), which is our Cross Entropy function\n",
    "\n",
    "And we use classic SGD optimizer to find minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CE_loss=torch.nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define batch iterative function\n",
    "\n",
    "This function is called for each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_batch(idx):\n",
    "\timgs, labels = next(iter(dataloader))\n",
    "\timgs = imgs.flatten(start_dim=1)\n",
    "\toptimizer.zero_grad()\n",
    "\ty_model=model(imgs.float())\n",
    "\n",
    "\tloss=CE_loss(y_model,labels)\n",
    "\tloss.backward()\n",
    "\toptimizer.step()\n",
    "\n",
    "\tpredicted_labels = y_model.argmax(dim=1)\n",
    "\tacc = (predicted_labels == labels).sum()/len(labels)\n",
    "\treturn loss.detach(), acc.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of batches: \n",
      " 6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6000 [00:00<?, ?it/s]C:\\Users\\Shlomi\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n",
      "100%|██████████| 6000/6000 [00:07<00:00, 795.31it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "batches=len(dataloader)\n",
    "print(\"num of batches: \\n\", batches)\n",
    "batch_loss=torch.zeros(batches)\n",
    "batch_acc=torch.zeros(batches)\n",
    "for idx in tqdm(range(batches)):\n",
    "\tbatch_loss[idx], batch_acc[idx] = iterate_batch(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Block diagram of the NN\n",
    "\n",
    "![](./drawio/Untitled%20Diagram.drawio.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15720\n",
      "15720\n"
     ]
    }
   ],
   "source": [
    "def get_n_params(model):\n",
    "    pp=0\n",
    "    for p in list(model.parameters()):\n",
    "        nn=1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(get_n_params(model))\n",
    "print(count_parameters(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual calculation\n",
    "\n",
    "```\n",
    "Hidden layer:\n",
    "\n",
    "392*20 + 392*20 = 7840 weights\n",
    "Also, we have 20 output neurons: 20 neurons means 20 biases (one bias for each Y_0, Y_1, ..., Y_19 equation)\n",
    "\n",
    "Activation layer:\n",
    "0 (no weights, no bias, ReLU is max(0, X) so no additional parameters except from the input to the function itself)\n",
    "\n",
    "\n",
    "Summary:\n",
    "7840 + 0 + 20 = 7860\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compared to built-in layers of PyTorch\n",
    "\n",
    "It's the same, but I calculated manually diffirently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15720\n",
      "15720\n"
     ]
    }
   ],
   "source": [
    "torch_model = torch.nn.Sequential(\n",
    "\ttorch.nn.Linear(392, 20),\n",
    "\ttorch.nn.Softmax()\n",
    ")\n",
    "print(get_n_params(model))\n",
    "print(count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------+\n",
      "|     Modules      | Parameters |\n",
      "+------------------+------------+\n",
      "|    0.weights     |    7840    |\n",
      "|      0.bias      |     20     |\n",
      "| 0.linear1.weight |    7840    |\n",
      "|  0.linear1.bias  |     20     |\n",
      "+------------------+------------+\n",
      "Total Trainable Params: 15720\n",
      "Sequential(\n",
      "  (0): MyCustomLayer(\n",
      "    (linear1): Linear(in_features=392, out_features=20, bias=True)\n",
      "    (activation): ReLU(inplace=True)\n",
      "  )\n",
      "  (1): Softmax(dim=None)\n",
      ")\n",
      "+----------+------------+\n",
      "| Modules  | Parameters |\n",
      "+----------+------------+\n",
      "| 0.weight |    7840    |\n",
      "|  0.bias  |     20     |\n",
      "+----------+------------+\n",
      "Total Trainable Params: 7860\n",
      "Sequential(\n",
      "  (0): Linear(in_features=392, out_features=20, bias=True)\n",
      "  (1): Softmax(dim=None)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params+=params\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "\n",
    "count_parameters(model)\n",
    "print(model)\n",
    "\n",
    "count_parameters(torch_model)\n",
    "print(torch_model)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "491bb19b02bca4120e598d98c213a4bec2319d7b10161d80e88d4a71a12afc06"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
