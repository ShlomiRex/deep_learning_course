{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sklearn.datasets as skds\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import numpy\n",
    "import pandas\n",
    "from typing import Optional\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropNorm(nn.Module):\n",
    "\tdef __init__(self, size_in, size_out):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tself.size_in, self.size_out = size_in, size_out\n",
    "\n",
    "\t\tself.weights = nn.Parameter(torch.Tensor(size_out, size_in))  # nn.Parameter is a Tensor that's a module parameter.\n",
    "\t\tself.bias = nn.Parameter(torch.Tensor(size_out))\n",
    "\n",
    "\t\tself.linear1 = nn.Linear(size_in, size_out)\n",
    "\t\tself.activation = nn.ReLU(inplace=True) # inplace = don't use extra memory\n",
    "\n",
    "\t\tself.drop_probability = 0.5  # We can take as argument but this question doesn't requires it.\n",
    "\t\t\n",
    "\t\n",
    "\tdef forward(self, batch):\n",
    "\t\t# First dim = batch size\n",
    "\t\t# Second dim = sample\n",
    "\t\tprint(\"Batch size:\")\n",
    "\t\tprint(batch.size())\n",
    "\n",
    "\t\tsingle_img = torch.squeeze(batch[0]).detach().reshape(28, 28) \n",
    "\n",
    "\t\tmask = torch.bernoulli(batch, self.drop_probability)\n",
    "\t\t#mask = torch.flatten(mask)\n",
    "\n",
    "\t\t# a = torch.randn([10, 80, 2])\n",
    "\t\t# b = torch.randint(0, 2, (10,))\n",
    "\t\t# res = a[b!=1]\n",
    "\n",
    "\t\tzeros = torch.zeros(self.size_in)\n",
    "\t\tzeros = zeros + mask * batch\n",
    "\n",
    "\t\timg_after_drop = torch.squeeze(zeros[0]).detach().reshape(28, 28)\n",
    "\n",
    "\n",
    "\t\t# Draw image afer drop\n",
    "\t\tfig = plt.figure()\n",
    "\t\tax = fig.add_subplot(2, 2, 0+1)\n",
    "\t\tplt.imshow(single_img,cmap='Greys')\n",
    "\t\tax.set_title(\"Before drop\")\n",
    "\t\tax.axes.get_xaxis().set_visible(False)\n",
    "\t\tax.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "\t\tax = fig.add_subplot(2, 2, 1+1)\n",
    "\t\tplt.imshow(img_after_drop,cmap='Greys')\n",
    "\t\tax.set_title(\"After drop\")\n",
    "\t\tax.axes.get_xaxis().set_visible(False)\n",
    "\t\tax.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "\n",
    "\n",
    "\t\tbatch_dropped = batch * mask\n",
    "\n",
    "\t\treturn batch\n",
    "\n",
    "# model = nn.Sequential(\n",
    "# \tnn.Linear(784, 2049),\n",
    "# \tnn.ReLU(),\n",
    "\n",
    "# \tDropNorm(2049, 512),\n",
    "# \tnn.ReLU(),\n",
    "\n",
    "# \tnn.Linear(512, 20),\n",
    "# \tnn.Softmax()\n",
    "# )\n",
    "\n",
    "model = nn.Sequential(\n",
    "\tDropNorm(784, 512),\n",
    "\n",
    "\tnn.Linear(512, 2049),\n",
    "\tnn.ReLU(),\n",
    "\n",
    "\tnn.Linear(2049, 512),\n",
    "\tnn.ReLU(),\n",
    "\n",
    "\tnn.Linear(512, 20),\n",
    "\tnn.Softmax()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of samples: 60000\n",
      "batch size:  10\n",
      "num of batches: 6000\n",
      "image size: torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "\n",
    "\n",
    "dataset = torchvision.datasets.FashionMNIST(root=\"/22961\", train=True, download=True, transform=torchvision.transforms.PILToTensor())\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "print(\"num of samples:\", len(dataset))\n",
    "batches=len(dataloader)\n",
    "print(\"batch size: \", batch_size)\n",
    "print(\"num of batches:\", batches)\n",
    "print(\"image size:\", dataset[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func=torch.nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_batch(idx):\n",
    "\timgs, labels = next(iter(dataloader))\n",
    "\timgs = imgs.flatten(start_dim=1)\n",
    "\n",
    "\toptimizer.zero_grad()\n",
    "\ty_model=model(imgs.float())\n",
    "\n",
    "\tloss=loss_func(y_model,labels)\n",
    "\tloss.backward()\n",
    "\toptimizer.step()\n",
    "\n",
    "\tpredicted_labels = y_model.argmax(dim=1)\n",
    "\tacc = (predicted_labels == labels).sum()/len(labels)\n",
    "\treturn loss.detach(), acc.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Computing on GPU (if possible)\n",
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "get_default_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size:\n",
      "torch.Size([10, 784])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (10x784 and 512x2049)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23476/1623438728.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mbatch_acc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mbatch_loss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_acc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miterate_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23476/2330221368.py\u001b[0m in \u001b[0;36miterate_batch\u001b[1;34m(idx)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0my_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Shlomi\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Shlomi\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Shlomi\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Shlomi\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Shlomi\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1846\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (10x784 and 512x2049)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "batch_loss=torch.zeros(batches)\n",
    "batch_acc=torch.zeros(batches)\n",
    "for idx in tqdm(range(batches)):\n",
    "\tbatch_loss[idx], batch_acc[idx] = iterate_batch(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(range(1,batches+1), batch_loss,label=\"Train Loss\")\n",
    "plt.title(\"CE loss\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch Number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0].size()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bf879a411aebe39b4058fa858b0b44b05480a906f645312f900a603c4f6baa26"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
