{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sklearn.datasets as skds\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import numpy\n",
    "import pandas\n",
    "from typing import Optional\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1 A - basic code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: \n",
      " tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])\n",
      "x-mu: \n",
      " tensor([-4.5000, -3.5000, -2.5000, -1.5000, -0.5000,  0.5000,  1.5000,  2.5000,\n",
      "         3.5000,  4.5000])\n",
      "(x-mu)/a: \n",
      " tensor([-1.4863, -1.1560, -0.8257, -0.4954, -0.1651,  0.1651,  0.4954,  0.8257,\n",
      "         1.1560,  1.4863])\n",
      "mask: \n",
      " tensor([1., 0., 0., 0., 0., 1., 0., 1., 0., 1.])\n",
      "after mask and calculations: \n",
      " tensor([-1.4863, -0.0000, -0.0000, -0.0000, -0.0000,  0.1651,  0.0000,  0.8257,\n",
      "         0.0000,  1.4863])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(0, 10, dtype=torch.float)\n",
    "\n",
    "mask = torch.bernoulli(x, 0.5)  # 0 and 1 distrobution\n",
    "# negative_mask = torch.tensor((mask == 0), dtype=torch.uint8)  # Opposite of mask\n",
    "\n",
    "# Y = mask * a  # Apply mask \n",
    "# Y\n",
    "\n",
    "mu = x.mean() # Mean\n",
    "sigma = x.std() # Standard deviation\n",
    "epsilon = .00001 # To avoid division by zero\n",
    "\n",
    "a = torch.sqrt((sigma ** 2) + epsilon)\n",
    "\n",
    "calc = (x - mu) / a \n",
    "\n",
    "print(\"x: \\n\", x)\n",
    "print(\"x-mu: \\n\", x-mu)\n",
    "print(\"(x-mu)/a: \\n\", (x-mu)/a)\n",
    "print(\"mask: \\n\", mask)\n",
    "\n",
    "x = torch.mul(calc, mask)\n",
    "\n",
    "\n",
    "\n",
    "print(\"after mask and calculations: \\n\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropNorm(nn.Module):\n",
    "\tdef __init__(self, size_in, size_out):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tself.size_in, self.size_out = size_in, size_out\n",
    "\n",
    "\t\tself.weights = nn.Parameter(torch.Tensor(size_out, size_in))  # nn.Parameter is a Tensor that's a module parameter.\n",
    "\t\tself.bias = nn.Parameter(torch.Tensor(size_out))\n",
    "\n",
    "\t\tself.linear1 = nn.Linear(size_in, size_out)\n",
    "\t\tself.activation = nn.ReLU(inplace=True) # inplace = don't use extra memory\n",
    "\n",
    "\t\tself.drop_probability = 0.5  # We can take as argument but this question doesn't requires it.\n",
    "\t\t\n",
    "\t\n",
    "\tdef forward(self, batch):\n",
    "\t\t# First dim = batch size\n",
    "\t\t# Second dim = sample\n",
    "\n",
    "\t\tmu = batch.mean(dim=1) # Mean\n",
    "\t\tsigma = batch.std(dim=1) # Standard deviation\n",
    "\t\tepsilon = .00001 # To avoid division by zero\n",
    "\t\tY = torch.sqrt((sigma ** 2) + epsilon)\n",
    "\n",
    "\t\t# batch: [600, 2049]\n",
    "\t\t# mu: [600]\n",
    "\t\t# We need to substract mu from each dimension of batch.\n",
    "\n",
    "\t\t# (batch[0] - mu[0]) is like: [2049] - [1]\n",
    "\n",
    "\t\t#batch[0] = batch[0] - mu[0]\n",
    "\t\t#batch[1] = batch[1] - mu[1]\n",
    "\t\t# ...\n",
    "\n",
    "\t\tnew_x = (batch - mu) / Y\n",
    "\n",
    "\t\treturn new_x\n",
    "\n",
    "model = nn.Sequential(\n",
    "\tnn.Linear(784, 2049),\n",
    "\tnn.ReLU(),\n",
    "\n",
    "\tDropNorm(2049, 2049),\n",
    "\tnn.ReLU(),\n",
    "\n",
    "\tnn.Linear(2049, 20),\n",
    "\tnn.Softmax()\n",
    ")\n",
    "\n",
    "# model = nn.Sequential(\n",
    "# \tDropNorm(784, 784),\n",
    "\n",
    "# \tnn.Linear(784, 2049),\n",
    "# \tnn.ReLU(),\n",
    "\n",
    "# \tnn.Linear(2049, 512),\n",
    "# \tnn.ReLU(),\n",
    "\n",
    "# \tnn.Linear(512, 20),\n",
    "# \tnn.Softmax()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of samples: 60000\n",
      "batch size:  600\n",
      "num of batches: 100\n",
      "image size: torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 600\n",
    "\n",
    "\n",
    "dataset = torchvision.datasets.FashionMNIST(root=\"/22961\", train=True, download=True, transform=torchvision.transforms.PILToTensor())\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "print(\"num of samples:\", len(dataset))\n",
    "batches=len(dataloader)\n",
    "print(\"batch size: \", batch_size)\n",
    "print(\"num of batches:\", batches)\n",
    "print(\"image size:\", dataset[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func=torch.nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_batch(idx):\n",
    "\timgs, labels = next(iter(dataloader))\n",
    "\timgs = imgs.flatten(start_dim=1)\n",
    "\n",
    "\toptimizer.zero_grad()\n",
    "\ty_model=model(imgs.float())\n",
    "\n",
    "\tloss=loss_func(y_model,labels)\n",
    "\tloss.backward()\n",
    "\toptimizer.step()\n",
    "\n",
    "\tpredicted_labels = y_model.argmax(dim=1)\n",
    "\tacc = (predicted_labels == labels).sum()/len(labels)\n",
    "\treturn loss.detach(), acc.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Computing on GPU (if possible)\n",
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "get_default_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "batch_loss=torch.zeros(batches)\n",
    "batch_acc=torch.zeros(batches)\n",
    "for idx in tqdm(range(batches)):\n",
    "\tbatch_loss[idx], batch_acc[idx] = iterate_batch(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(range(1,batches+1), batch_loss,label=\"Train Loss\")\n",
    "plt.title(\"Loss\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch Number\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bf879a411aebe39b4058fa858b0b44b05480a906f645312f900a603c4f6baa26"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
