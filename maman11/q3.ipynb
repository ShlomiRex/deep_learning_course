{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import tensor\n",
    "from typing import List, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MyScalar class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyScalar:\n",
    "\tdef __init__(self, value, gradient = 1, parent = None):\n",
    "\t\tself.value = value\n",
    "\t\tself.gradient = gradient\n",
    "\t\tself.parent = parent\n",
    "\n",
    "\tdef __repr__(self):\n",
    "\t\treturn f\"MyScalar({self.value}, {self.gradient}, {self.parent})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_add(scalar: MyScalar, n):\n",
    "    value = scalar.value + n\n",
    "    grad = 1\n",
    "    return MyScalar(value, grad, scalar)\n",
    "\n",
    "def my_mul(scalar: MyScalar, n):\n",
    "    value = n * scalar.value\n",
    "    grad = n\n",
    "    return MyScalar(value, grad, scalar)\n",
    "\n",
    "def my_pow(scalar: MyScalar, n):\n",
    "    value = scalar.value ** n\n",
    "    grad = n * (scalar.value ** (n-1))\n",
    "    return MyScalar(value, grad, scalar)\n",
    "\n",
    "def my_cos(scalar: MyScalar):\n",
    "    value = torch.cos(scalar.value)\n",
    "    grad = -torch.sin(scalar.value)\n",
    "    return MyScalar(value, grad, scalar)\n",
    "\n",
    "def my_sin(scalar: MyScalar):\n",
    "    value = torch.sin(scalar.value)\n",
    "    grad = torch.cos(scalar.value)\n",
    "    return MyScalar(value, grad, scalar)\n",
    "\n",
    "def my_ln(scalar: MyScalar):\n",
    "    value = torch.log(scalar.value)\n",
    "    grad = 1/(scalar.value)\n",
    "    return MyScalar(value, grad, scalar)\n",
    "\n",
    "def my_exp(scalar: MyScalar):\n",
    "\t# e^a when 'a' is scalar, is 1*e^a = e^a\n",
    "    value = torch.exp(torch.tensor(scalar.value))\n",
    "    grad = torch.exp(torch.tensor(scalar.value))\n",
    "    return MyScalar(value, grad, scalar)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "global grad_graph\n",
    "grad_graph = {}\n",
    "\n",
    "def get_gradient(a: MyScalar, cumgrad=1):\n",
    "\tif a.parent is None:\n",
    "\t\tgrad_graph[a] = cumgrad\n",
    "\telse:\n",
    "\t\tgrad_graph[a] = cumgrad\n",
    "\t\tget_gradient(a.parent, a.gradient * cumgrad)\n",
    "\treturn grad_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{MyScalar(5, 1, None): 1}\n",
      "tensor([1.]) 5.0\n"
     ]
    }
   ],
   "source": [
    "grad_graph = {}  # Like .grad = None\n",
    "my_x = MyScalar(5) # x = 5 -> value = 5, gradient = 1, parent=None\n",
    "# Gradient = 1 because (df/dx)(x) = 1\n",
    "print(get_gradient(my_x))\n",
    "\n",
    "x = torch.tensor([5.], requires_grad=True)\n",
    "x.backward()\n",
    "print(x.grad, x.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{MyScalar(15, 3, MyScalar(5, 1, None)): 1, MyScalar(5, 1, None): 3}\n"
     ]
    }
   ],
   "source": [
    "grad_graph = {}\n",
    "my_x = MyScalar(5) # x = 5, value = 5, gradient = 1, parent=None\n",
    "my_y = my_mul(my_x, 3) # y = 3x , value = 3*5 = 15, gradient = 3, parent=my_x\n",
    "print(get_gradient(my_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`z = e^(a^2)`\n",
    "\n",
    "\n",
    "`dz/da = dz*(a^2)/da * dz*e^(a^2)`\n",
    "\n",
    "\n",
    "`dz/da = 2a * e^(a^2) = 4 * 54.598 = 218.4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{MyScalar(54.59815216064453, 54.59815216064453, MyScalar(4, 4, MyScalar(2, 1, None))): 1, MyScalar(4, 4, MyScalar(2, 1, None)): tensor(54.5982), MyScalar(2, 1, None): tensor(218.3926)}\n",
      "\n",
      "\n",
      " tensor([218.3926])\n"
     ]
    }
   ],
   "source": [
    "grad_graph = {} \n",
    "a=MyScalar(2) # a = 2, value = 2, gradient = 1, parent=None\n",
    "b=my_pow(a,2) # b=a^2, value = 4, gradient = 2a = 4, parent=a\n",
    "c=my_exp(b) # c=e^(a^2) = e^4 = 54.59815..., value = 54.5918, gradient = 54.5918, parent=b\n",
    "d=get_gradient(c)\n",
    "print(d)\n",
    "\n",
    "torch_a = torch.tensor([2.], requires_grad=True)\n",
    "torch_b = torch.pow(torch_a, 2)\n",
    "torch_c = torch.exp(torch_b)\n",
    "torch_c.backward()\n",
    "print(\"\\n\\n\", torch_a.grad)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
